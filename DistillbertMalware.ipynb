{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoqgOR22mlZUA9EFFDz8Ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msorrentino3/Tirocinio/blob/main/DistillbertMalware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "metadata": {
        "id": "OOgEjSCMcLgf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#caricamento del dataset\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "#creo nuova colonna nel dataset dove metto 1 se nella colonna 3 compare attack altrimenti metto 0\n",
        "data[\"label\"] = np.where(data[\"class3\"] == \"Attack\", 1, 0)\n",
        "\n",
        "#creo testo per addestare modello distillbert\n",
        "#converto ogni colonna nel formato string e se nella cella della colonna del dataset risulta\n",
        "#senza dato verr√† sostituito con stringa vuota\n",
        "scr = data[\"Scr_IP\"].astype(str).replace(\"nan\", \"\")\n",
        "des = data[\"Des_IP\"].astype(str).replace(\"nan\", \"\")\n",
        "proto = data[\"Protocol\"].astype(str).replace(\"nan\", \"\")\n",
        "serv = data[\"Service\"].astype(str).replace(\"nan\", \"\")\n",
        "alert = data[\"OSSEC_alert\"].astype(str).replace(\"nan\", \"\")\n",
        "\n",
        "#creo nel dataset colonna nella quale metto sottoforma di string il contenuto della riga\n",
        "data[\"text\"] = pd.Series(\n",
        "    np.where(scr != \"\", \"Da \" + scr + \" \", \"\") +\n",
        "    np.where(des != \"\", \"A \" + des + \" \", \"\") +\n",
        "    np.where(proto != \"\", \"Protocollo \" + proto + \" \", \"\") +\n",
        "    np.where(serv != \"\", \"Servizio \" + serv + \" \", \"\") +\n",
        "    np.where((alert != \"\") & (alert != \"0\"), \"Alert \" + alert, \"\")\n",
        ").str.strip()\n",
        "\n",
        "#divido il set di dati per addestrare il modello (60% dei dati) e sia per testarlo(40%)\n",
        "train_df, test_df = train_test_split(\n",
        "    data[[\"text\", \"label\"]],\n",
        "    test_size=0.4,\n",
        "    stratify=data[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "data = pd.read_csv(\"data.csv\")"
      ],
      "metadata": {
        "id": "nSIciPcaoaqi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizzo sia i dati di addestramento che quelli di test\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "train_enc = tokenizer(list(train_df[\"text\"]), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "test_enc  = tokenizer(list(test_df[\"text\"]),  padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Aggiungo le etichette\n",
        "train_enc['labels'] = torch.tensor(train_df[\"label\"].values)\n",
        "test_enc['labels']  = torch.tensor(test_df[\"label\"].values)\n",
        "\n",
        "train_dataset = TensorDataset(train_enc['input_ids'], train_enc['attention_mask'], train_enc['labels'])\n",
        "test_dataset  = TensorDataset(test_enc['input_ids'],  test_enc['attention_mask'],  test_enc['labels'])\n",
        "\n",
        "#creazione dataset e dataloder\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, pin_memory=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D3XVkRuozVt",
        "outputId": "de6f0ae4-9bc8-494f-9b92-da39bc505e79"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seleziono se utilizzare la gpu se √® libera oppure presente nel dispositivo altrimenti utilizza la gpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#scarico modello pre-addestrato di ditillbert ingrado di fare delle classificazioni\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2 #utilizzo lables = 2 perch√® devo fare due tipi di classificazione (attacco e normale)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) #ottimizzo i parametri e passo la velocit√† con cui il modello impara\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BigvnZ8No7i9",
        "outputId": "5d2867cb-466b-4e39-f732-1e1adc7614f3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Addestro il modello\n",
        "model.train()\n",
        "epoche = 3 #scelto 3 perch√® √® cos√¨ ho prestazioni superiori al 90% anche\n",
        "for ep in range(epoche): #il modello scansiona i dati per essere addestrato x il numero di epoche\n",
        "    tot_loss = 0\n",
        "    for input_ids, attention_mask, labels in train_loader: #prendo i dati per l'allenamento\n",
        "        input_ids = input_ids.to(device, non_blocking=True) #sposto il testo tokenizzato sulla gpu per rendere pi√π veloce l'esecuzione\n",
        "        attention_mask = attention_mask.to(device, non_blocking=True) #spsoto nella gpu sullo le parti di testo che non sono rimpieti con il padding\n",
        "        labels = labels.to(device, non_blocking=True) #porto sulla gpu i valori per indicare se si tratta di un attacco o meno\n",
        "        optimizer.zero_grad() #azzero i parametri cos√¨ nella prossima epoca i valori non si sommino\n",
        "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss #il modello legge il testo e cerca di capire se si tratta di un attacco o meno\n",
        "        #infine calcola quanto ha sbagliato\n",
        "        loss.backward() #calcola di quanto devono essere aggiornati i pesi dei parametri per la prossima epoca\n",
        "        optimizer.step()#cambio i pesi dei parametri (il modello ha \"imparato\")\n",
        "\n",
        "        tot_loss += loss.item() #calcolo la media dell'errore alla fine dell'epoca\n",
        "    print(f\"Epoca {ep+1}: Loss media = {tot_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGFn-MEIpC5e",
        "outputId": "90437b2f-3ef9-49db-b07f-7299cd945eba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1: Loss media = 0.6050\n",
            "Epoca 2: Loss media = 0.2994\n",
            "Epoca 3: Loss media = 0.1581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#valuto il modello\n",
        "model.eval() #setto il modello in mdalit√† valutazione cos√¨ che non aggiorni pi√π i pesi dei parametri\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_loader:#prendo i dati per effettuare il test del modello\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits #il modello effettua la previsione\n",
        "        preds = logits.argmax(dim=1) #sceglie se si tratta di un attacco oppure no\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy()) #salva le predizioni e le sposta sulla cpu in formato numpy\n",
        "        all_labels.extend(labels.cpu().numpy())#salva i dati del batch e li sposta sulla cpu in formato numpy\n",
        "\n",
        "# Calcolo metriche\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "print(f\"\\nAccuracy:  {accuracy*100:.2f}%\")\n",
        "print(f\"Precision: {precision*100:.2f}%\")\n",
        "print(f\"Recall:    {recall*100:.2f}%\")\n",
        "print(f\"F1-score:  {f1*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUwc_XFtpIf6",
        "outputId": "deb7e1e1-3b6c-4eed-924b-ef6aed94d788"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy:  95.00%\n",
            "Precision: 93.60%\n",
            "Recall:    96.45%\n",
            "F1-score:  95.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test svolto per capire se il modello √® funzionante o meno\n",
        "def predici(text):\n",
        "    x = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**x).logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        label = probs.argmax().item()\n",
        "    return (\"ATTACK\" if label else \"NORMAL\"), float(probs[0][label])\n",
        "\n",
        "esempi = [\n",
        "    \"Da: 10.0.1.5 | A: 131.236.3.92 | Protocollo: udp | Servizio: dns\",\n",
        "    \"Da: 192.168.2.199 | A: 192.168.2.10 | Protocollo: tcp | Servizio: http | Alert: TRUE\",\n",
        "    \"Da: 172.24.1.80 | A: 172.24.1.1 | Protocollo: udp | Servizio: dns\"\n",
        "]\n",
        "\n",
        "for i, testo in enumerate(esempi, 1):\n",
        "    pred, conf = predici(testo)\n",
        "    print(f\"\\nüì° Esempio {i}:\")\n",
        "    print(f\"Traffico: {testo}\")\n",
        "    print(f\"Predizione: {pred} ({conf*100:.1f}% sicuro)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w03buxNGpTLn",
        "outputId": "b3abafd0-f9cf-4d73-ddbe-9f3bd9c09cb0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì° Esempio 1:\n",
            "Traffico: Da: 10.0.1.5 | A: 131.236.3.92 | Protocollo: udp | Servizio: dns\n",
            "Predizione: NORMAL (97.5% sicuro)\n",
            "\n",
            "üì° Esempio 2:\n",
            "Traffico: Da: 192.168.2.199 | A: 192.168.2.10 | Protocollo: tcp | Servizio: http | Alert: TRUE\n",
            "Predizione: ATTACK (87.5% sicuro)\n",
            "\n",
            "üì° Esempio 3:\n",
            "Traffico: Da: 172.24.1.80 | A: 172.24.1.1 | Protocollo: udp | Servizio: dns\n",
            "Predizione: NORMAL (97.4% sicuro)\n"
          ]
        }
      ]
    }
  ]
}